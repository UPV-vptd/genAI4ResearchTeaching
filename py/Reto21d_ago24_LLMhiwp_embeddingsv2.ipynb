{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMiO3u+csdjFhB+mxyzgJs3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jamg-upv/0Reto21d_ago24_LLMclassification/blob/main/py/Reto21d_ago24_LLMhiwp_embeddingsv2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "En principio este codigo funciona con un entorno de de computacion de 12GB RAM sin GPU.\n",
        "si se atascara por RAM puedo probarlo a partirlo en bloques y que vaya actualizando un fichero de resultados\n",
        "En ultima instancia me planteo compara bloques de computación para poder usar un GPU o entornos más intensivos de RAM"
      ],
      "metadata": {
        "id": "spIdvUXwcWiE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Preparacion previa (ejecutar solo una vez)\n"
      ],
      "metadata": {
        "id": "X8FGO3RO7rWU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Conexion a espacios"
      ],
      "metadata": {
        "id": "DthBQ_fkXc3-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#conectar con repositorio github publico para acceso a los datasets\n",
        "!git clone https://github.com/jamg-upv/0Reto21d_ago24_LLMclassification.git\n",
        "\n",
        "#conecar a google drive para guardar alli los outputs que quiera mantener (y que no se me olvide descargarlos) luego los subiré a Git para su uso posterior\n",
        "from google.colab import drive\n",
        "import os\n",
        "\n",
        "# Montar Google Drive si aún no está montado\n",
        "if not os.path.exists('/content/drive'):\n",
        "    drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SD9kl1-cn-AW",
        "outputId": "11ce5488-eb08-424e-d76a-2b7d066ec4c9"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into '0Reto21d_ago24_LLMclassification'...\n",
            "remote: Enumerating objects: 72, done.\u001b[K\n",
            "remote: Counting objects: 100% (72/72), done.\u001b[K\n",
            "remote: Compressing objects: 100% (67/67), done.\u001b[K\n",
            "remote: Total 72 (delta 31), reused 0 (delta 0), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (72/72), 1.91 MiB | 5.38 MiB/s, done.\n",
            "Resolving deltas: 100% (31/31), done.\n",
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Parametrización de variables"
      ],
      "metadata": {
        "id": "xvXuHnNnSZZA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define el nombre del archivo y el path de salida\n",
        "input_file_name = 'hiwp_componentsv2.csv'\n",
        "# estructura del archivo de datos:\n",
        "# Component\tDescription\tType\tClass\n",
        "\n",
        "input_path = '/content/0Reto21d_ago24_LLMclassification/datasets/'\n",
        "\n",
        "output_path = '/content/drive/MyDrive/Reto21dias_24/'\n",
        "\n",
        "# Construye el path completo\n",
        "full_in_path = os.path.join(input_path, input_file_name)\n",
        "full_in_path\n",
        "# full_out_path = os.path.join(output_path, file_name)\n",
        "\n",
        "# Define la variable con el nombre de la columna\n",
        "target_column = 'Description'\n",
        "\n",
        "# Objetos a clasificar\n",
        "selec_objects = ['art']\n",
        "\n",
        "#categorias en la que quiero clasificar los objetos\n",
        "selec_categories = ['cat1', 'cat2','cat4']\n",
        "\n"
      ],
      "metadata": {
        "id": "WVOoxtMCSpux"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cargar el archivo de datos y filtrar los objetos a clasificar y las categorias a utilizar para la clasificación\n",
        "\n",
        "Crea el DataFrame objects filtrando hiwp_data para incluir solo las filas donde el valor en la columna 'Type' está en la lista selec_objects.\n",
        "Crea el DataFrame categories filtrando hiwp_data para incluir solo las filas donde el valor en la columna 'Type' está en la lista selec_categories.\n",
        "\n",
        "Si necesito que la comparación no sea sensible a mayúsculas/minúsculas, puedes modificar el código así:\n",
        "\n",
        "```\n",
        "objects = hiwp_data[hiwp_data['Type'].str.lower().isin([x.lower() for x in selec_objects])]\n",
        "categories = hiwp_data[hiwp_data['Type'].str.lower().isin([x.lower() for x in selec_categories])]\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "E9vQnWUeSm1X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "# Leer el archivo CSV (es un CSV de Open Office si lo genero con MSexcel no se mapea bien sin dar atributos adicionales)\n",
        "hiwp_data = pd.read_csv(full_in_path)\n",
        "\n",
        "# Mostrar el DataFrame resultante\n",
        "# print(hiwp_data)\n",
        "\n",
        "# Crear el DataFrame 'objects'\n",
        "objects = hiwp_data[hiwp_data['Type'].isin(selec_objects)]\n",
        "\n",
        "# Crear el DataFrame 'categories'\n",
        "categories = hiwp_data[hiwp_data['Type'].isin(selec_categories)]\n",
        "\n",
        "# Opción 1: Reconstruir el índice descartando el índice original\n",
        "# objects= objects.reset_index(drop=True)\n",
        "# categories= categories.reset_index(drop=True)\n",
        "\n",
        "# Opción 2: Reconstruir el índice manteniendo el índice original como una nueva columna\n",
        "objects= objects.reset_index()\n",
        "categories= categories.reset_index()\n",
        "\n",
        "\n",
        "# Verificar los resultados\n",
        "print(\"DataFrame 'objects':\")\n",
        "print(objects.shape)\n",
        "print(objects['Type'].value_counts())\n",
        "print(\"\\nPrimeras filas de 'objects':\")\n",
        "print(objects.head())\n",
        "\n",
        "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
        "\n",
        "print(\"DataFrame 'categories':\")\n",
        "print(categories.shape)\n",
        "print(categories['Type'].value_counts())\n",
        "print(\"\\nPrimeras filas de 'categories':\")\n",
        "print(categories.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hkStor1tSnr3",
        "outputId": "267a32ca-e6d4-4b7b-bc58-7871edc440bc"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DataFrame 'objects':\n",
            "(7, 5)\n",
            "Type\n",
            "art    7\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Primeras filas de 'objects':\n",
            "   index     Component                                        Description  \\\n",
            "0     18     Hauff2022  High-performance work practices, employee well...   \n",
            "1     19      Song2021  High involvement work systems and organization...   \n",
            "2     20    Marin 2016  Deconstructing AMO framework: a systematic rev...   \n",
            "3     21     Ahmad2022  The impact of green HRM on green creativity: m...   \n",
            "4     22  Houeland2022  Not my task': Role perceptions in a green tran...   \n",
            "\n",
            "  Type Class  \n",
            "0  art     1  \n",
            "1  art     1  \n",
            "2  art     1  \n",
            "3  art     0  \n",
            "4  art     0  \n",
            "\n",
            "==================================================\n",
            "\n",
            "DataFrame 'categories':\n",
            "(17, 5)\n",
            "Type\n",
            "cat1    12\n",
            "cat4     4\n",
            "cat2     1\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Primeras filas de 'categories':\n",
            "   index           Component  \\\n",
            "0      0  Select&Recruitment   \n",
            "1      1            Training   \n",
            "2      2         Empowerment   \n",
            "3      3         PerformEval   \n",
            "4      4       Compensations   \n",
            "\n",
            "                                         Description  Type  Class  \n",
            "0  Rigorous Selection and Recruitment Selecting t...  cat1   long  \n",
            "1  Continuous Training and Development Investing ...  cat1   long  \n",
            "2  Employee participation Autonomy and Empowermen...  cat1   long  \n",
            "3  Performance Evaluation and Feedback Providing ...  cat1  short  \n",
            "4  Competitive Compensation and Benefits Offering...  cat1   long  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Embeedings\n",
        "Quiero hacer tareas de vectorizacion con google collab y dispongo de 12gb de ram (sin GPU) salvo que pase a la versión de pago. La tarea que quiero hacer es vectorizar/embeedings celdas de texto (de unas 200 palabras como máximo cada una). Todos los textos son en inglés. los embbedings los quiero utilizar para una tarea de clasificación de textos cientificos (la entrada es titulo y abstracr en la celda a vectorizar) y tengo varias categorias (con una definicion de la categoria que tambien querría vectorizar) y luego calcular la distancia entre cada articulo y cada categoria\n",
        "\n"
      ],
      "metadata": {
        "id": "Djbdr5_wJvRe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "pendiente\n",
        "\n",
        "* guardar los embeedings en un csv de cada uno de los modelos (con el nombre que toque. decidir si son un csv por modelo  a una tabla donde estén todos juntos por filas (ojo que cada uno tendrá una dimensión distinta). mejor una excel y cada uno una pestaña diferente y así lo puedo leer cada vez\n",
        "* idem con guardar las distancias de coseno\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "4E0gWCZAZkzK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # Instalar las dependencias necesarias\n",
        "import pkg_resources\n",
        "import subprocess\n",
        "\n",
        "required_packages = [\n",
        "    'seaborn',\n",
        "    'matplotlib',\n",
        "    'sentence-transformers',\n",
        "    'pandas',\n",
        "    'numpy',\n",
        "    'transformers',\n",
        "    'torch'\n",
        "]\n",
        "\n",
        "installed_packages = {pkg.key for pkg in pkg_resources.working_set}\n",
        "\n",
        "for package in required_packages:\n",
        "    if package in installed_packages:\n",
        "        print(f\"{package} ya está instalado.\")\n",
        "    else:\n",
        "        print(f\"{package} no está instalado. Instalando...\")\n",
        "        subprocess.check_call([\"pip\", \"install\", package])\n",
        "        print(f\"{package} ha sido instalado.\")\n",
        "\n",
        "print(\"\\nTodas las bibliotecas necesarias están ahora instaladas.\")\n",
        "\n",
        "\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import time\n",
        "import torch\n",
        "from datetime import datetime"
      ],
      "metadata": {
        "id": "LJD6g8svhpM5"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Obtener la fecha y hora actual para los nombres de archivo\n",
        "current_time = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "\n",
        "# Lista de modelos a probar (con los ejemplos ha llegado a un tope de 6gb de ram)\n",
        "models_to_test = [\n",
        "    'allenai-specter',\n",
        "    'all-MiniLM-L6-v2',\n",
        "    # 'all-mpnet-base-v2',\n",
        "    # 'paraphrase-multilingual-mpnet-base-v2',\n",
        "    # 'distiluse-base-multilingual-cased-v1',\n",
        "    # 'allenai/scibert_scivocab_uncased',\n",
        "    # 'distilbert-base-nli-mean-tokens',\n",
        "    # 'roberta-base-nli-stsb-mean-tokens',\n",
        "    # 'all-distilroberta-v1',\n",
        "    # 'all-MiniLM-L12-v2',\n",
        "    # 'distiluse-base-multilingual-cased-v2',\n",
        "    # 'paraphrase-multilingual-MiniLM-L12-v2',\n",
        "    # 'stsb-roberta-large',\n",
        "    # 'bert-base-nli-mean-tokens'\n",
        "]\n",
        "\n",
        "# Función para evaluar un modelo\n",
        "def evaluate_model(model_name, objects_df, categories_df):\n",
        "    print(f\"Evaluating model: {model_name}\")\n",
        "    model = SentenceTransformer(model_name)\n",
        "\n",
        "    start_time = time.time()\n",
        "\n",
        "    # Generar embeddings para objetos\n",
        "    object_texts = objects_df['Description'].tolist()\n",
        "    object_embeddings = model.encode(object_texts, show_progress_bar=True)\n",
        "\n",
        "    # Generar embeddings para categorías\n",
        "    category_texts = categories_df['Description'].tolist()\n",
        "    category_embeddings = model.encode(category_texts, show_progress_bar=True)\n",
        "\n",
        "    # Calcular similitud coseno\n",
        "    similarity_matrix = util.cos_sim(object_embeddings, category_embeddings)\n",
        "\n",
        "    # Encontrar las mejores coincidencias\n",
        "    best_matches = np.argmax(similarity_matrix, axis=1)\n",
        "\n",
        "    end_time = time.time()\n",
        "    processing_time = end_time - start_time\n",
        "\n",
        "    return {\n",
        "        'model': model_name,\n",
        "        'best_matches': categories_df.iloc[best_matches]['Component'].tolist(),\n",
        "        'processing_time': round(processing_time, 2),  # Redondear a 2 decimales\n",
        "        'embedding_size': object_embeddings.shape[1],\n",
        "        'similarity_matrix': similarity_matrix,\n",
        "        'object_embeddings': object_embeddings,\n",
        "        'category_embeddings': category_embeddings\n",
        "    }\n",
        "\n",
        "# Evaluar todos los modelos\n",
        "results = []\n",
        "similarity_data = []\n",
        "embeddings_data = []\n",
        "\n",
        "for model_name in models_to_test:\n",
        "    try:\n",
        "        result = evaluate_model(model_name, objects, categories)\n",
        "        results.append({\n",
        "            'model': result['model'],\n",
        "            'best_matches': result['best_matches'],\n",
        "            'processing_time': result['processing_time'],\n",
        "            'embedding_size': result['embedding_size']\n",
        "        })\n",
        "\n",
        "        # Preparar datos de similitud para este modelo\n",
        "        for i, obj in enumerate(objects['Component']):\n",
        "            row = [model_name, obj] + result['similarity_matrix'][i].tolist()\n",
        "            similarity_data.append(row)\n",
        "\n",
        "        # Preparar datos de embeddings para este modelo\n",
        "        for i, obj in enumerate(objects['Description']):\n",
        "            embeddings_data.append([model_name, objects['Component'].iloc[i], obj, result['object_embeddings'][i].tolist()])\n",
        "        for i, cat in enumerate(categories['Description']):\n",
        "            embeddings_data.append([model_name, categories['Component'].iloc[i], cat, result['category_embeddings'][i].tolist()])\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error evaluating {model_name}: {str(e)}\")\n",
        "\n",
        "# Crear DataFrame con resultados\n",
        "df_results = pd.DataFrame(results)\n",
        "df_results = df_results.rename(columns={'processing_time': 'processing_time_seconds'})\n",
        "print(df_results)\n",
        "\n",
        "# Guardar resultados\n",
        "# Asegurarse de que la carpeta de salida existe\n",
        "os.makedirs(output_path, exist_ok=True)\n",
        "\n",
        "# Guardar resultados\n",
        "results_path = os.path.join(output_path, f'model_comparison_results_{current_time}.csv')\n",
        "df_results.to_csv(results_path, index=False)\n",
        "print(f\"Resultados de comparación guardados en {results_path}\")\n",
        "\n",
        "# Crear DataFrame de similitud coseno\n",
        "columns = ['Model', 'Object'] + categories['Component'].tolist()\n",
        "df_similarity = pd.DataFrame(similarity_data, columns=columns)\n",
        "\n",
        "# Guardar DataFrame de similitud coseno\n",
        "similarity_path = os.path.join(output_path, f'tablas_similitud_coseno_{current_time}.csv')\n",
        "df_similarity.to_csv(similarity_path, index=False)\n",
        "print(f\"Tablas de similitud coseno guardadas en {similarity_path}\")\n",
        "\n",
        "# Crear DataFrame de embeddings\n",
        "df_embeddings = pd.DataFrame(embeddings_data, columns=['Model', 'Component', 'Description', 'Embedding'])\n",
        "\n",
        "# Guardar DataFrame de embeddings\n",
        "embeddings_path = os.path.join(output_path, f'embeddings_{current_time}.csv')\n",
        "df_embeddings.to_csv(embeddings_path, index=False)\n",
        "print(f\"Embeddings guardados en {embeddings_path}\")\n",
        "\n",
        "\n",
        "# Crear y guardar la nueva tabla de similitud para HIWPshortDescription\n",
        "df_hiwp_similarity = df_similarity[['Model', 'Object', 'HIWPshortDescrip']].copy()\n",
        "df_hiwp_similarity.columns = ['Model', 'Object_Description', 'Similarity']\n",
        "df_hiwp_similarity['Object_Component'] = objects['Component'].tolist() * len(models_to_test)\n",
        "df_hiwp_similarity = df_hiwp_similarity[['Model', 'Object_Component', 'Object_Description', 'Similarity']]\n",
        "df_hiwp_similarity = df_hiwp_similarity.sort_values(['Model', 'Similarity'], ascending=[True, False])\n",
        "\n",
        "hiwp_similarity_path = os.path.join(output_path, f'hiwp_similarity_{current_time}.csv')\n",
        "df_hiwp_similarity.to_csv(hiwp_similarity_path, index=False)\n",
        "print(f\"Tabla de similitud para HIWPshortDescrip guardada en {hiwp_similarity_path}\")"
      ],
      "metadata": {
        "id": "52Ci32ItW0xd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b3e3c134-9499-4e8c-f7db-0ddf7807dacf"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tabla de similitud para HIWPshortDescrip guardada en /content/drive/MyDrive/Reto21dias_24/hiwp_similarity_20240813_215630.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Calcula el rango de similitud para cada modelo individualmente, donde el rango 1 es el mejor (mayor similitud).\n",
        "# Calcula el rango promedio para cada Object_Description a través de todos los modelos.\n",
        "# Ordena los objetos por su rango promedio (el más bajo es el mejor).\n",
        "# Guarda esta tabla resumen simplificada en un nuevo archivo CSV.\n",
        "# La tabla resumen final contendrá las siguientes columnas:\n",
        "# # Object_Component: El componente del objeto.\n",
        "# # Object_Description: La descripción del objeto.\n",
        "# # Average_Rank: El rango promedio del objeto a través de todos los modelos (más bajo es mejor).\n",
        "# Los objetos estarán ordenados por su rango promedio,\n",
        "# lo que  dará una visión integrada de cómo se comportan a través de todos los modelos\n",
        "# en relación con HIWPshortDescription.\n",
        "# No es sencillo poner un punto de corte a partir del cual se consideran \"excluidos\" los objetos a clasificar.\n",
        "# de modo que se haría un escreening por titulo y abstract mnual.\n",
        "# partienod del orden de esta tabla hasta llegar aun  punto donde\n",
        "# haya varios seguidos sin seleccionar. Momento en el que se podría parar el screening asistido por\n",
        "# clasificacion automática.\n",
        "\n",
        "# Función para calcular el rango inverso (el más alto obtiene el rango 1)\n",
        "def inverse_rank(series):\n",
        "    return series.rank(ascending=False, method='min')\n",
        "\n",
        "# Calcular rangos para cada modelo\n",
        "df_ranks = df_hiwp_similarity.groupby('Model').apply(lambda x: x.assign(Rank=inverse_rank(x['Similarity']))).reset_index(drop=True)\n",
        "\n",
        "# Calcular el rango promedio para cada Object_Component a través de todos los modelos\n",
        "df_summary = df_ranks.groupby('Object_Component')['Rank'].mean().reset_index()\n",
        "df_summary = df_summary.rename(columns={'Rank': 'Average_Rank'})\n",
        "\n",
        "# Añadir la Description correspondiente a cada Component\n",
        "df_summary = df_summary.merge(objects[['Component', 'Description']], left_on='Object_Component', right_on='Component', how='left')\n",
        "\n",
        "# Ordenar por rango promedio (ascendente, ya que el rango más bajo es mejor)\n",
        "df_summary = df_summary.sort_values('Average_Rank')\n",
        "\n",
        "# Reordenar las columnas\n",
        "df_summary = df_summary[['Object_Component', 'Description', 'Average_Rank']]\n",
        "\n",
        "# Guardar la tabla resumen\n",
        "summary_path = os.path.join(output_path, f'hiwp_similarity_summary_{current_time}.csv')\n",
        "df_summary.to_csv(summary_path, index=False)\n",
        "print(f\"Tabla resumen de similitud para HIWPshortDescription guardada en {summary_path}\")\n",
        "\n",
        "# Mostrar las primeras filas de la tabla resumen\n",
        "print(df_summary.head(10))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AFNE3e7c6NAJ",
        "outputId": "ea5fbe06-c15a-4d25-a589-ee97613011ce"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tabla resumen de similitud para HIWPshortDescription guardada en /content/drive/MyDrive/Reto21dias_24/hiwp_similarity_summary_20240813_215630.csv\n",
            "  Object_Component                                        Description  \\\n",
            "6         Song2021  High involvement work systems and organization...   \n",
            "3        Hauff2022  High-performance work practices, employee well...   \n",
            "5       Marin 2016  Deconstructing AMO framework: a systematic rev...   \n",
            "2      Burbano2022  Mitigating Gig and Remote Worker Misconduct: E...   \n",
            "0        Ahmad2022  The impact of green HRM on green creativity: m...   \n",
            "1       Becker2022  Surviving remotely: How job control and loneli...   \n",
            "4     Houeland2022  Not my task': Role perceptions in a green tran...   \n",
            "\n",
            "   Average_Rank  \n",
            "6           1.0  \n",
            "3           2.5  \n",
            "5           2.5  \n",
            "2           4.0  \n",
            "0           5.5  \n",
            "1           5.5  \n",
            "4           7.0  \n"
          ]
        }
      ]
    }
  ]
}